{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicción sobre lealtad de clientes de BetaBank\n",
    "\n",
    "# Obetivo: Crear un modelo confiable que prediga si un cliente dejará al Banco\n",
    "\n",
    "## Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos y librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           9091 non-null   float64\n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
      "0          1    15634602  Hargrave          619    France  Female   42   \n",
      "1          2    15647311      Hill          608     Spain  Female   41   \n",
      "2          3    15619304      Onio          502    France  Female   42   \n",
      "3          4    15701354      Boni          699    France  Female   39   \n",
      "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
      "\n",
      "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
      "0     2.0       0.00              1          1               1   \n",
      "1     1.0   83807.86              1          0               1   \n",
      "2     8.0  159660.80              3          1               0   \n",
      "3     1.0       0.00              2          0               0   \n",
      "4     2.0  125510.82              1          1               1   \n",
      "\n",
      "   EstimatedSalary  Exited  \n",
      "0        101348.88       1  \n",
      "1        112542.58       0  \n",
      "2        113931.57       1  \n",
      "3         93826.63       0  \n",
      "4         79084.10       0  \n"
     ]
    }
   ],
   "source": [
    "# Vamos a cargar los datos a utilizar\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "data = pd.read_csv('Churn.csv')\n",
    "\n",
    "print(data.info())\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    RowNumber  CustomerId    Surname  CreditScore Geography  Gender  Age  \\\n",
      "30         31    15589475    Azikiwe          591     Spain  Female   39   \n",
      "48         49    15766205        Yin          550   Germany    Male   38   \n",
      "51         52    15768193  Trevisani          585   Germany    Male   36   \n",
      "53         54    15702298   Parkhill          655   Germany    Male   41   \n",
      "60         61    15651280     Hunter          742   Germany    Male   35   \n",
      "\n",
      "    Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
      "30     NaN       0.00              3          1               0   \n",
      "48     NaN  103391.38              1          0               1   \n",
      "51     NaN  146050.97              2          0               0   \n",
      "53     NaN  125561.97              1          0               0   \n",
      "60     NaN  136857.00              1          0               0   \n",
      "\n",
      "    EstimatedSalary  Exited  \n",
      "30        140469.38       1  \n",
      "48         90878.13       0  \n",
      "51         86424.57       0  \n",
      "53        164040.94       1  \n",
      "60         84509.57       0  \n",
      "IsActiveMember\n",
      "1    464\n",
      "0    445\n",
      "Name: count, dtype: int64\n",
      "Exited\n",
      "0    726\n",
      "1    183\n",
      "Name: count, dtype: int64\n",
      "Geography\n",
      "France     464\n",
      "Spain      229\n",
      "Germany    216\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Vamos a trabajar con la columna 'Tenure' que tiene valores ausentes; debemos buscar que hacer con esos valores ausentes\n",
    "\n",
    "data_null = data[data['Tenure'].isna()]\n",
    "print(data_null.head())\n",
    "print(data_null['IsActiveMember'].value_counts(dropna=False))\n",
    "print(data_null['Exited'].value_counts(dropna=False))\n",
    "print(data_null['Geography'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento de valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.901833872707659\n",
      "5.022246787342822\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           10000 non-null  float64\n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Vamos a calcular la media de Tenure para llenar aquellas celdas con valores ausentes\n",
    "\n",
    "ternure_mean_1 = (data[(~data['Tenure'].isna())&(data['Exited']==1)]['Tenure']).mean()\n",
    "ternure_mean_0 = (data[(~data['Tenure'].isna())&(data['Exited']==0)]['Tenure']).mean()\n",
    "print(ternure_mean_1)\n",
    "print(ternure_mean_0)\n",
    "\n",
    "data['Tenure'] = data['Tenure'].fillna(ternure_mean_0)\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del contenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exited\n",
      "0    3117\n",
      "1     500\n",
      "Name: count, dtype: int64\n",
      "IsActiveMember\n",
      "1    1873\n",
      "0    1744\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Vamos a ver la información de los usuarios donde la columna \"Balance\" es 0\n",
    "print((data[data['Balance']==0])['Exited'].value_counts())\n",
    "print((data[data['Balance']==0])['IsActiveMember'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   RowNumber          10000 non-null  int64  \n",
      " 1   CustomerId         10000 non-null  int64  \n",
      " 2   CreditScore        10000 non-null  int64  \n",
      " 3   Age                10000 non-null  int64  \n",
      " 4   Tenure             10000 non-null  float64\n",
      " 5   Balance            10000 non-null  float64\n",
      " 6   NumOfProducts      10000 non-null  int64  \n",
      " 7   HasCrCard          10000 non-null  int64  \n",
      " 8   IsActiveMember     10000 non-null  int64  \n",
      " 9   EstimatedSalary    10000 non-null  float64\n",
      " 10  Exited             10000 non-null  int64  \n",
      " 11  Geography_Germany  10000 non-null  bool   \n",
      " 12  Geography_Spain    10000 non-null  bool   \n",
      " 13  Gender_Male        10000 non-null  bool   \n",
      "dtypes: bool(3), float64(3), int64(8)\n",
      "memory usage: 888.8 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Vamos a transformar nuestras caracteristicas categóricas en características numéricas para poder entrenar nuestro modelo\n",
    "data_ohe = data.drop('Surname', axis=1)\n",
    "#categories = ['Geography','Gender']\n",
    "data_ohe = pd.get_dummies(data_ohe, drop_first=True)\n",
    "print(data_ohe.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primera conclusión \n",
    "\n",
    "Antes de comenzar cualquier análisis, entrenamiento o predicción, bemeos de asegurar que nuestros datos son adecuados para comenzar con estas actividades y que entendemos el contenido y estructura de nuestra fuente datos y la información que nos da.\n",
    "\n",
    "Por eso, en esta primera parte vimos que la columna \"Tenure\" presentaba poco menos de 1,000 celdas con valores nulos. Lo más sencillo era eliminar aquellas filas, sin embargo nos iba a hacer perder casi un 10% de la información de nuestro data set fuente. Por eso fue que opte por calcular la media de los valores de la columna 'Tenure' y rellenar esas celdas con valores nulos por la media de las demás obersvaciones. También se validó que, tomando en cuenta el contexto del caso, me parecio un poco extraño que tuvieramos Usuarios con Balance 0.0, por lo que quise encontrar alguna coincidencia con otras columnas, por ejemplo que auqellos usuarios con Balnce 0 fuese Usuarios que ya habiand ejado al banco o que no estuvieran activos. Sin embargo, no encontre alguna relación entre estas columnas y no realicé ningun otro procesamiento con los datos.\n",
    "\n",
    "Por último, antes de seguir, implemente la codificación OHE para las columnas de variables categóricas como \"Geography\" y \"Gender\" or lo que se crearon 3 columnas con la información de la nueva categorización. Sin embargo, para la columa 'Surname' opté por eliminarla de nuestro dataset ya que al implementar la codificación OHE nos iba a crear un data set con una gran cantidad de columnas y el costo computacional seria muy alto una vez que comencemos a entrenar a nuestros modelos. Además, ahora no nos arroja nada de información, salvo el identificar al usuario pero en ese caso también podemos usar la columna 'Customer ID'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equilibrio de clases y primer entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estandarización de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7500, 13)\n",
      "      RowNumber  CustomerId  CreditScore       Age    Tenure   Balance  \\\n",
      "226         227    15774393     0.442805 -0.841274  1.445348 -1.224577   \n",
      "7756       7757    15606232    -0.310897 -0.270730  0.718347  0.641783   \n",
      "2065       2066    15581840    -0.259274 -0.556002  1.081847 -1.224577   \n",
      "2800       2801    15646817     1.217157  1.155631  1.445348  1.290462   \n",
      "7028       7029    15618410     0.690598 -1.221637 -0.000568  1.142121   \n",
      "\n",
      "      NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  \\\n",
      "226        0.817772          1               1        -1.269750   \n",
      "7756      -0.896874          1               1         0.960396   \n",
      "2065       0.817772          1               0         0.661864   \n",
      "2800       0.817772          1               0        -1.039476   \n",
      "7028      -0.896874          0               0        -0.851729   \n",
      "\n",
      "      Geography_Germany  Geography_Spain  Gender_Male  \n",
      "226               False            False        False  \n",
      "7756              False             True        False  \n",
      "2065              False            False         True  \n",
      "2800              False            False         True  \n",
      "7028               True            False         True  \n",
      "(2500, 13)\n",
      "      RowNumber  CustomerId  CreditScore       Age    Tenure   Balance  \\\n",
      "7867       7868    15697201    -0.114728  0.680178 -0.735656 -1.224577   \n",
      "1402       1403    15613282     1.093261 -0.936365  1.081847  0.865861   \n",
      "8606       8607    15694581     1.609496  0.299815 -0.008655 -1.224577   \n",
      "8885       8886    15815125     0.174363  0.585087 -0.372156  0.419555   \n",
      "6494       6495    15752846     0.494429 -1.031456  0.718347 -1.224577   \n",
      "\n",
      "      NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  \\\n",
      "7867      -0.896874          1               1         0.969342   \n",
      "1402      -0.896874          1               0        -0.395081   \n",
      "8606       0.817772          1               1        -0.439560   \n",
      "8885       0.817772          1               1         1.006040   \n",
      "6494       0.817772          1               1        -1.343789   \n",
      "\n",
      "      Geography_Germany  Geography_Spain  Gender_Male  \n",
      "7867              False             True        False  \n",
      "1402              False            False         True  \n",
      "8606              False             True         True  \n",
      "8885              False             True         True  \n",
      "6494              False            False         True  \n"
     ]
    }
   ],
   "source": [
    "# Vamos a crear nuestros conjuntos de entrenamiento y de validación para el entrenamiento de nuestro modelo\n",
    "# Vamos a estandarizas los valores de nuestras caracteristicas numéricas para evitar un sesgo del modelo\n",
    "\n",
    "target = data_ohe['Exited']\n",
    "features = data_ohe.drop('Exited', axis=1)\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(features, target, test_size=0.25, random_state=12345)\n",
    "\n",
    "numeric = ['CreditScore','Age','Tenure','Balance','NumOfProducts','EstimatedSalary']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train[numeric])\n",
    "features_train[numeric] = scaler.transform(features_train[numeric])\n",
    "features_valid[numeric] = scaler.transform(features_valid[numeric])\n",
    "\n",
    "print(features_train.shape)\n",
    "print(features_train.head())\n",
    "print(features_valid.shape)\n",
    "print(features_valid.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanceo de clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajuste de pesos de clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1965    0]\n",
      " [ 535    0]]\n",
      "Recall del modelo = 0.0\n",
      "Precision del modelo = 0.0\n",
      "Valor F1 del modelo = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jrcis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Vamos a entrenar al modelo con Regresión Logistica y evaluarlo para validar en que nivel de precisión y recall andamos\n",
    "\n",
    "model = LogisticRegression(random_state=12345, solver='liblinear',)\n",
    "model.fit(features_train, target_train)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "print(confusion_matrix(target_valid, predicted_valid))\n",
    "print(\"Recall del modelo =\", recall_score(target_valid, predicted_valid))\n",
    "print(\"Precision del modelo =\", precision_score(target_valid, predicted_valid))\n",
    "print(\"Valor F1 del modelo =\", f1_score(target_valid, predicted_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1392  573]\n",
      " [ 159  376]]\n",
      "Recall del modelo = 0.702803738317757\n",
      "Precision del modelo = 0.39620653319283455\n",
      "Valor F1 del modelo = 0.5067385444743935\n"
     ]
    }
   ],
   "source": [
    "#Vamos a entrenar el mismo modelo pero usando el parametro class_weight para balancear las clases de la variable objetivo\n",
    "\n",
    "model = LogisticRegression(random_state=12345, solver='liblinear',class_weight='balanced')\n",
    "model.fit(features_train, target_train)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "print(confusion_matrix(target_valid, predicted_valid))\n",
    "print(\"Recall del modelo =\", recall_score(target_valid, predicted_valid))\n",
    "print(\"Precision del modelo =\", precision_score(target_valid, predicted_valid))\n",
    "print(\"Valor F1 del modelo =\", f1_score(target_valid, predicted_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sobremuestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12006, 13)\n",
      "(12006,)\n",
      "Exited\n",
      "1    6008\n",
      "0    5998\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Vamos a utilizar el método de Sobremuestreo para balancear las clases objetivo\n",
    "from sklearn.utils import shuffle\n",
    "def upsample(features, target, repeat):\n",
    "    features_zeros = features[target==0]\n",
    "    features_ones = features[target==1]\n",
    "    target_zeros = target[target==0]\n",
    "    target_ones = target[target==1]\n",
    "    \n",
    "    features_upsampled = pd.concat([features_zeros]+[features_ones]*repeat)\n",
    "    target_upsampled = pd.concat([target_zeros]+[target_ones]*repeat)\n",
    "    \n",
    "    features_upsampled, target_upsampled = shuffle(features_upsampled, target_upsampled, random_state=12345)\n",
    "    \n",
    "    return features_upsampled, target_upsampled\n",
    "\n",
    "features_upsampled, target_upsampled = upsample(features_train, target_train, 4)\n",
    "\n",
    "print(features_upsampled.shape)\n",
    "print(target_upsampled.shape)\n",
    "print(target_upsampled.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1001  964]\n",
      " [ 272  263]]\n",
      "Recall del modelo = 0.491588785046729\n",
      "Precision del modelo = 0.2143439282803586\n",
      "Valor F1 del modelo = 0.2985244040862656\n"
     ]
    }
   ],
   "source": [
    "#Vamos a entrenar el modelo con Sobremuestreo y calcular sus metricas\n",
    "\n",
    "model = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "model.fit(features_upsampled, target_upsampled)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "print(confusion_matrix(target_valid, predicted_valid))\n",
    "print(\"Recall del modelo =\", recall_score(target_valid, predicted_valid))\n",
    "print(\"Precision del modelo =\", precision_score(target_valid, predicted_valid))\n",
    "print(\"Valor F1 del modelo =\", f1_score(target_valid, predicted_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Umbral de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold = 0.00 | Precision = 0.214, Recall = 1.000\n",
      "Threshold = 0.02 | Precision = 0.214, Recall = 1.000\n",
      "Threshold = 0.04 | Precision = 0.214, Recall = 1.000\n",
      "Threshold = 0.06 | Precision = 0.214, Recall = 1.000\n",
      "Threshold = 0.08 | Precision = 0.214, Recall = 1.000\n",
      "Threshold = 0.10 | Precision = 0.214, Recall = 1.000\n",
      "Threshold = 0.12 | Precision = 0.214, Recall = 1.000\n",
      "Threshold = 0.14 | Precision = 0.214, Recall = 1.000\n",
      "Threshold = 0.16 | Precision = 0.214, Recall = 1.000\n",
      "Threshold = 0.18 | Precision = 0.214, Recall = 1.000\n",
      "Threshold = 0.20 | Precision = 0.224, Recall = 0.607\n",
      "Threshold = 0.22 | Precision = 0.000, Recall = 0.000\n",
      "Threshold = 0.24 | Precision = 0.000, Recall = 0.000\n",
      "Threshold = 0.26 | Precision = 0.000, Recall = 0.000\n",
      "Threshold = 0.28 | Precision = 0.000, Recall = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jrcis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\jrcis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\jrcis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\jrcis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Vamos a obtener las probabilidades para el umbral de clasificación para un modelo de Regresión Logística y vamos a obtener sus métricas\n",
    "model = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "model.fit(features_train, target_train)\n",
    "probabilities_valid = model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:,1]\n",
    "\n",
    "for threshold in np.arange(0,0.3,0.02):\n",
    "    predicted_valid = probabilities_one_valid > threshold\n",
    "    precision = precision_score(target_valid, predicted_valid)\n",
    "    recall = recall_score(target_valid, predicted_valid)\n",
    "    print('Threshold = {:.2f} | Precision = {:.3f}, Recall = {:.3f}'.format(threshold, precision, recall))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segunda conclusión\n",
    "\n",
    "Para esta segunda parte, hemos estandarizado los datos dado que nuestras variables numéricas tienen valores en distintas escalas y queremos evitar que nuestro modelo se llegue a sesgar o le de mayor importancia a una variable que a otra por sus valores. Es por eso que usamos la función Standard Scaler antesd e cualqueir entrenamiento\n",
    "\n",
    "Posteriormente, hemos notado que nuestra vairable objetivo 'Exited' esta desbalanceada. Para el conjunto de entrenamiento tenemos un 4:1 entre valores con '0' (cliente que no se han ido) y '1' (clientes que han dejado el banco). Necesitamos ejecutar técnicas de balanceo de clases para que nuestro modelo no sesgue sus predicciones después de su entrenamiento. ¿Como sabemos esto? Hemos corrido las predicciones, usando un modelo de Regresión Logistica, sin balancear las clases y los resultados de precisión y recall han sido de 0.0. ¡Inaceptable!\n",
    "\n",
    "-> Ajuste de peso de clases: Primero usamos el parametro 'class_weigth' en el modelo de Regresión Logistica, que realiza un balanceo de la clase objetivo durante el entrenamiento.\n",
    "\n",
    "-> Sobremuestreo: Segunda técnica, hemos creado nuevas observaciones que contengan '1' para nuestra variables objetivo para nivelar la cantidad de datos de las dos clases que tenemos.\n",
    "\n",
    "-> Tercera técnica, hemos calculado las probabilidades de las clases. Sin un ajuste de este umbral, la probabilidad será de 0.5 para cada clase, en este caso como buscamos tener más valores positivos dado que está desbalanceada la clase, se juega con la probabilidad en un rango de 0 a 0.3.\n",
    "\n",
    "Resultados: El ajuste de clases desde el parametro 'class_weigth' nos ha dado un mejor resultado al obtener un valor F1 de 0.5. Por su parte, en el ajuste de umbral, el mejor resutlado lo obtuvimos al setear una probabilidad de 0.3 para la clase negativa ya que onbtuvimos una precisión de 0.227 y un recall de 0.6. Por útlimo, el sobremuestreo nos arrojó un valor F1 DE 0.3, por debajo de el ajuste de peso de clases y del ajuste del umbral de probabilidades.\n",
    "\n",
    "Nos quedamos con el ajuste de pesos de clases 'class_weigth' para seguir ahora mejorando el modelo y obtener un valor F1 mayor a 0.6.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méjora del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árbol de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profundidad de arboles = 5\n",
      "Recall del modelo = 0.6934579439252336\n",
      "Precision del modelo = 0\n",
      "Valor F1 del modelo = 0.6008097165991902\n"
     ]
    }
   ],
   "source": [
    "# Vamos a entrenar a nuestro modelo usando Árboles de decisión\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "f1 = 0\n",
    "best_recall_score = 0\n",
    "best_precision_score = 0\n",
    "\n",
    "for trees in range (1, 20):\n",
    "    model = DecisionTreeClassifier(random_state=12345, max_depth=trees, class_weight = 'balanced')\n",
    "    model.fit(features_train, target_train)\n",
    "    answer = model.predict(features_valid)\n",
    "    f1_score_ = f1_score(target_valid, answer)\n",
    "    recall_score_ = recall_score(target_valid, answer)\n",
    "    precision_score_ = precision_score(target_valid, answer)\n",
    "    if f1_score_ > f1:\n",
    "        f1 = f1_score_\n",
    "        max_depth = trees\n",
    "        best_recall_score = recall_score_\n",
    "        best_precision_Score = precision_score\n",
    "\n",
    "print(\"Profundidad de arboles =\", max_depth)\n",
    "print(\"Recall del modelo =\", best_recall_score)\n",
    "print(\"Precision del modelo =\", best_precision_score)\n",
    "print(\"Valor F1 del modelo =\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bosque Aleatorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profundidad de arboles = 10\n",
      "Recall del modelo = 0.6299065420560748\n",
      "Precision del modelo = 0.6127272727272727\n",
      "Valor F1 del modelo = 0.6211981566820276\n"
     ]
    }
   ],
   "source": [
    "#Vamos a entrenar a nuestro modelo usando un Bosque Aleatorio y a probar con distintos ajustes de hiperparametros\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "f1 = 0\n",
    "i= 0\n",
    "best_recall_score = 0\n",
    "best_precision_score = 0\n",
    "\n",
    "for i in range(1,16):\n",
    "    model = RandomForestClassifier(random_state=12345, n_estimators = 10, max_depth = i, class_weight='balanced')\n",
    "    model.fit(features_train, target_train)\n",
    "    predicted_valid = model.predict(features_valid)\n",
    "    f1_score_tree = f1_score(target_valid, predicted_valid)\n",
    "    recall_score_tree = recall_score(target_valid, predicted_valid)\n",
    "    precision_score_tree = precision_score(target_valid, predicted_valid)\n",
    "    if f1_score_tree > f1:\n",
    "        f1 = f1_score_tree\n",
    "        max_depth = i\n",
    "        best_recall_score = recall_score_tree\n",
    "        best_precision_score = precision_score_tree\n",
    "\n",
    "print(\"Profundidad de arboles =\", max_depth)\n",
    "print(\"Recall del modelo =\", best_recall_score)\n",
    "print(\"Precision del modelo =\", best_precision_score)\n",
    "print(\"Valor F1 del modelo =\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor para hiperparametro n_estimators = 30\n",
      "Recall del modelo = 0.6299065420560748\n",
      "Precision del modelo = 0.6406844106463878\n",
      "Valor F1 del modelo = 0.6352497643732328\n"
     ]
    }
   ],
   "source": [
    "# Vamos a probar moviendonos sobre disinto ajuste para N_estimators\n",
    "\n",
    "f1 = 0\n",
    "best_recall_score = 0\n",
    "best_precision_score = 0\n",
    "\n",
    "for i in range(5,50, 5):\n",
    "    model = RandomForestClassifier(random_state=12345, n_estimators = i, max_depth = 10, class_weight='balanced')\n",
    "    model.fit(features_train, target_train)\n",
    "    predicted_valid = model.predict(features_valid)\n",
    "    f1_score_tree = f1_score(target_valid, predicted_valid)\n",
    "    recall_score_tree = recall_score(target_valid, predicted_valid)\n",
    "    precision_score_tree = precision_score(target_valid, predicted_valid)\n",
    "    if f1_score_tree > f1:\n",
    "        f1 = f1_score_tree\n",
    "        max_depth = i\n",
    "        best_recall_score = recall_score_tree\n",
    "        best_precision_score = precision_score_tree\n",
    "\n",
    "print(\"Valor para hiperparametro n_estimators =\", max_depth)\n",
    "print(\"Recall del modelo =\", best_recall_score)\n",
    "print(\"Precision del modelo =\", best_precision_score)\n",
    "print(\"Valor F1 del modelo =\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tercera conclusión\n",
    "\n",
    "Hemos probado disintos algoritmos de aprendizaje para nuestro modelo y hemos jugado con sus parametros para encontrar la mejor combinación de estos que nos ayudará a encontrar el mayor valor de F1, que es la media armonizada de la precisión y el recall.\n",
    "\n",
    "--> El módelo de Logistic Regression si bien es de un costo computacional bajo y se ejecuta de una manera muy rápida, al ejecutarlo sin un balanceo de clases nos arrroja un recall y precisión de 0,0. Al establecer el parametro weigth_class, mejora muchisimo al modelo. Sin embargo el valor F1 es de 0.5, se debe seguir mejorando.\n",
    "\n",
    "--> El modelo de Arbol de decisión, nos arroja un recall de 0.69, muy bueno, sin embargo la precisión s de 0.0. Una clara evidencia de que nuestro modelo esta SOBREAJUSTADO ya que nos dice que nuestro modelo es capaz de identificar muy bien los Valores Positivos entre todos los valores posibles pero no es capaz de hacer una buena predicción.\n",
    "\n",
    "--> Bosque Aleatorio ha sido nuestro MEJOR MODELO, ya que obtuvimos un valor F1 de 0.62 con valores de Recall y Precision muy similares. Estas metricas se alcanzaron usando hiperparametros como max_depth y n_estimators. Se realziaron bucles para encontrar los mejores valores para nuestros hiperparametros, la cantidad de estimadores se fijo en ser menor que 50 para evitar un costo computacional muy alto y la profundiad del modelo de estableció menor a 10 por el sobreajuste de nuestro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución del mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor modelo entrenado para determinar si los clientes de Beta Bank se irán o no tiene un recall de 0.6299065420560748, una precisión de 0.6346516007532956 y un valor F1 de 0.6322701688555347.\n",
      "El valor del área bajo la curva ROC, que va de 0 a 1, es de: 0.8636037192932391.\n"
     ]
    }
   ],
   "source": [
    "# Finalmente vamos a entrenar nuestro modelo con el algoritmo y parametros con los que obtuvimos mejores resultados\n",
    "\n",
    "model = RandomForestClassifier(random_state=12345, n_estimators = 40, max_depth=10, class_weight = 'balanced')\n",
    "model.fit(features_train, target_train)\n",
    "predictions = model.predict(features_valid)\n",
    "f1_score_final = f1_score(target_valid, predicted_valid)\n",
    "recall_score_final = recall_score(target_valid, predicted_valid)\n",
    "precision_score_final = precision_score(target_valid, predicted_valid)\n",
    "\n",
    "print(f\"El mejor modelo entrenado para determinar si los clientes de Beta Bank se irán o no tiene un recall de {recall_score_final}, una precisión de {precision_score_final} y un valor F1 de {f1_score_final}.\")\n",
    "\n",
    "# Por ultimo obtenemos el valor AUC-ROC (Area bajo la Curva) para valdiar los resultados de nuestro modelo\n",
    "\n",
    "probabilities = model.predict_proba(features_valid)\n",
    "probabilities_one = probabilities[:,1]\n",
    "\n",
    "auc_roc = roc_auc_score(target_valid, probabilities_one)\n",
    "print(f\"El valor del área bajo la curva ROC, que va de 0 a 1, es de: {auc_roc}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión Final\n",
    "\n",
    "¡Los resultados de nuestro modelo han sido satisfactorios!\n",
    "\n",
    "Para esto hemos utilizado un algoritmo de aprendizaje de Bosque Aleatorio para nuestro modelo que presentaba un sobreajuste y un desbalanceo de clases en un principio. Temas que se abordaron con el uso de hiperparametros como class_weight, n_estimators y max_depth.\n",
    "\n",
    "El valor de F1 es de 0.62 para nuestro conjunto de validación, mientras que el valor de AUC-ROC es de 0.86, valor que está por encima del 0.5, lo que coincide con los valores de precisión y recall que tenemos en nuestro modelo (Verdaderos Positivos vs Flalsos Positivos)\n",
    "\n",
    "Ahora podemos ejecutar nuestro modelo para poder encontrar a aquellos clientes que, con un 62% de precisiómn, podemos decir que se irán del banco. Y para ellos, nuestro equipo de marketing podra trabajar en una estrtaegia especifica para abordar aquellas necesidades o carencias que tienen en su servicio y podamos revertir una potencial decisión de huida de capital del banco Beta Bank"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 502,
    "start_time": "2025-02-15T17:36:28.780Z"
   },
   {
    "duration": 38,
    "start_time": "2025-02-15T17:36:33.817Z"
   },
   {
    "duration": 7,
    "start_time": "2025-02-15T17:55:30.197Z"
   },
   {
    "duration": 9,
    "start_time": "2025-02-15T17:57:05.160Z"
   },
   {
    "duration": 8,
    "start_time": "2025-02-15T17:57:44.281Z"
   },
   {
    "duration": 10,
    "start_time": "2025-02-15T17:58:10.984Z"
   },
   {
    "duration": 11,
    "start_time": "2025-02-15T17:59:38.757Z"
   },
   {
    "duration": 14,
    "start_time": "2025-02-15T18:18:14.647Z"
   },
   {
    "duration": 13,
    "start_time": "2025-02-15T18:20:22.405Z"
   },
   {
    "duration": 8,
    "start_time": "2025-02-15T18:21:31.780Z"
   },
   {
    "duration": 15,
    "start_time": "2025-02-15T18:21:56.330Z"
   },
   {
    "duration": 5,
    "start_time": "2025-02-15T18:23:44.283Z"
   },
   {
    "duration": 18,
    "start_time": "2025-02-15T18:24:08.683Z"
   },
   {
    "duration": 7,
    "start_time": "2025-02-15T18:24:20.485Z"
   },
   {
    "duration": 18,
    "start_time": "2025-02-15T18:32:08.755Z"
   },
   {
    "duration": 13,
    "start_time": "2025-02-15T18:32:14.042Z"
   },
   {
    "duration": 11,
    "start_time": "2025-02-15T18:33:25.756Z"
   },
   {
    "duration": 5,
    "start_time": "2025-02-15T18:34:15.581Z"
   },
   {
    "duration": 8,
    "start_time": "2025-02-15T18:35:15.627Z"
   },
   {
    "duration": 467,
    "start_time": "2025-02-15T19:03:31.948Z"
   },
   {
    "duration": 10,
    "start_time": "2025-02-15T19:03:57.560Z"
   },
   {
    "duration": 14,
    "start_time": "2025-02-15T19:04:06.608Z"
   },
   {
    "duration": 7,
    "start_time": "2025-02-15T19:04:11.854Z"
   },
   {
    "duration": 72,
    "start_time": "2025-02-15T19:04:15.946Z"
   },
   {
    "duration": 29,
    "start_time": "2025-02-15T19:04:48.956Z"
   },
   {
    "duration": 49,
    "start_time": "2025-02-15T19:25:37.696Z"
   },
   {
    "duration": 59,
    "start_time": "2025-02-15T19:46:22.926Z"
   },
   {
    "duration": 127,
    "start_time": "2025-02-15T19:46:44.188Z"
   },
   {
    "duration": 405,
    "start_time": "2025-02-15T19:51:40.965Z"
   },
   {
    "duration": 784,
    "start_time": "2025-02-15T19:51:58.467Z"
   },
   {
    "duration": 12,
    "start_time": "2025-02-15T19:51:59.253Z"
   },
   {
    "duration": 13,
    "start_time": "2025-02-15T19:51:59.267Z"
   },
   {
    "duration": 19,
    "start_time": "2025-02-15T19:51:59.282Z"
   },
   {
    "duration": 12,
    "start_time": "2025-02-15T19:51:59.317Z"
   },
   {
    "duration": 707,
    "start_time": "2025-02-15T19:51:59.331Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-15T19:52:00.041Z"
   },
   {
    "duration": 33,
    "start_time": "2025-02-15T19:53:19.344Z"
   },
   {
    "duration": 941,
    "start_time": "2025-02-15T19:53:27.014Z"
   },
   {
    "duration": 10,
    "start_time": "2025-02-15T19:53:27.958Z"
   },
   {
    "duration": 26,
    "start_time": "2025-02-15T19:53:27.971Z"
   },
   {
    "duration": 7,
    "start_time": "2025-02-15T19:53:27.999Z"
   },
   {
    "duration": 586,
    "start_time": "2025-02-15T19:53:28.008Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-15T19:53:28.596Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-15T19:53:28.597Z"
   },
   {
    "duration": 15,
    "start_time": "2025-02-15T20:00:33.407Z"
   },
   {
    "duration": 34,
    "start_time": "2025-02-15T20:01:28.500Z"
   },
   {
    "duration": 193,
    "start_time": "2025-02-15T20:01:38.797Z"
   },
   {
    "duration": 814,
    "start_time": "2025-02-15T20:02:25.257Z"
   },
   {
    "duration": 9,
    "start_time": "2025-02-15T20:02:26.074Z"
   },
   {
    "duration": 15,
    "start_time": "2025-02-15T20:02:26.085Z"
   },
   {
    "duration": 7,
    "start_time": "2025-02-15T20:02:26.103Z"
   },
   {
    "duration": 15,
    "start_time": "2025-02-15T20:02:26.113Z"
   },
   {
    "duration": 27,
    "start_time": "2025-02-15T20:02:26.130Z"
   },
   {
    "duration": 145,
    "start_time": "2025-02-15T20:02:26.159Z"
   },
   {
    "duration": 793,
    "start_time": "2025-02-15T20:05:15.909Z"
   },
   {
    "duration": 10,
    "start_time": "2025-02-15T20:05:16.704Z"
   },
   {
    "duration": 13,
    "start_time": "2025-02-15T20:05:16.716Z"
   },
   {
    "duration": 7,
    "start_time": "2025-02-15T20:05:16.731Z"
   },
   {
    "duration": 17,
    "start_time": "2025-02-15T20:05:16.740Z"
   },
   {
    "duration": 51,
    "start_time": "2025-02-15T20:05:16.759Z"
   },
   {
    "duration": 176,
    "start_time": "2025-02-15T20:05:16.812Z"
   },
   {
    "duration": 758,
    "start_time": "2025-02-15T20:08:42.149Z"
   },
   {
    "duration": 10,
    "start_time": "2025-02-15T20:08:42.910Z"
   },
   {
    "duration": 14,
    "start_time": "2025-02-15T20:08:42.922Z"
   },
   {
    "duration": 6,
    "start_time": "2025-02-15T20:08:42.939Z"
   },
   {
    "duration": 37,
    "start_time": "2025-02-15T20:08:42.948Z"
   },
   {
    "duration": 27,
    "start_time": "2025-02-15T20:08:42.987Z"
   },
   {
    "duration": 73,
    "start_time": "2025-02-15T20:08:43.016Z"
   },
   {
    "duration": 758,
    "start_time": "2025-02-15T20:09:41.550Z"
   },
   {
    "duration": 9,
    "start_time": "2025-02-15T20:09:42.312Z"
   },
   {
    "duration": 12,
    "start_time": "2025-02-15T20:09:42.323Z"
   },
   {
    "duration": 6,
    "start_time": "2025-02-15T20:09:42.337Z"
   },
   {
    "duration": 15,
    "start_time": "2025-02-15T20:09:42.345Z"
   },
   {
    "duration": 34,
    "start_time": "2025-02-15T20:09:42.362Z"
   },
   {
    "duration": 45,
    "start_time": "2025-02-15T20:09:42.398Z"
   },
   {
    "duration": 61,
    "start_time": "2025-02-15T20:12:41.628Z"
   },
   {
    "duration": 813,
    "start_time": "2025-02-15T21:07:41.310Z"
   },
   {
    "duration": 10,
    "start_time": "2025-02-15T21:07:42.126Z"
   },
   {
    "duration": 14,
    "start_time": "2025-02-15T21:07:42.138Z"
   },
   {
    "duration": 6,
    "start_time": "2025-02-15T21:07:42.154Z"
   },
   {
    "duration": 36,
    "start_time": "2025-02-15T21:07:42.162Z"
   },
   {
    "duration": 28,
    "start_time": "2025-02-15T21:07:42.200Z"
   },
   {
    "duration": 83,
    "start_time": "2025-02-15T21:07:42.230Z"
   },
   {
    "duration": 5,
    "start_time": "2025-02-15T21:17:45.941Z"
   },
   {
    "duration": 6,
    "start_time": "2025-02-15T21:17:57.419Z"
   },
   {
    "duration": 7,
    "start_time": "2025-02-15T21:18:37.905Z"
   },
   {
    "duration": 238,
    "start_time": "2025-02-15T21:24:21.510Z"
   },
   {
    "duration": 15,
    "start_time": "2025-02-15T21:24:30.810Z"
   },
   {
    "duration": 15,
    "start_time": "2025-02-15T21:25:19.802Z"
   },
   {
    "duration": 12,
    "start_time": "2025-02-15T21:25:47.246Z"
   },
   {
    "duration": 26,
    "start_time": "2025-02-15T21:26:41.557Z"
   },
   {
    "duration": 12,
    "start_time": "2025-02-15T21:26:50.819Z"
   },
   {
    "duration": 11,
    "start_time": "2025-02-15T21:27:00.461Z"
   },
   {
    "duration": 13,
    "start_time": "2025-02-15T21:27:10.897Z"
   },
   {
    "duration": 11,
    "start_time": "2025-02-15T21:27:18.165Z"
   },
   {
    "duration": 21,
    "start_time": "2025-02-15T21:29:16.856Z"
   },
   {
    "duration": 813,
    "start_time": "2025-02-15T21:31:39.093Z"
   },
   {
    "duration": 9,
    "start_time": "2025-02-15T21:31:39.908Z"
   },
   {
    "duration": 12,
    "start_time": "2025-02-15T21:31:39.919Z"
   },
   {
    "duration": 6,
    "start_time": "2025-02-15T21:31:39.933Z"
   },
   {
    "duration": 15,
    "start_time": "2025-02-15T21:31:39.941Z"
   },
   {
    "duration": 54,
    "start_time": "2025-02-15T21:31:39.957Z"
   },
   {
    "duration": 78,
    "start_time": "2025-02-15T21:31:40.013Z"
   },
   {
    "duration": 13,
    "start_time": "2025-02-15T21:31:40.093Z"
   },
   {
    "duration": 95,
    "start_time": "2025-02-15T21:31:40.108Z"
   },
   {
    "duration": 14,
    "start_time": "2025-02-15T21:32:43.747Z"
   },
   {
    "duration": 28,
    "start_time": "2025-02-15T21:32:46.135Z"
   },
   {
    "duration": 14,
    "start_time": "2025-02-15T21:33:03.091Z"
   },
   {
    "duration": 20,
    "start_time": "2025-02-15T21:33:04.996Z"
   },
   {
    "duration": 12,
    "start_time": "2025-02-15T21:33:12.176Z"
   },
   {
    "duration": 40,
    "start_time": "2025-02-15T21:33:15.390Z"
   },
   {
    "duration": 163,
    "start_time": "2025-02-16T19:07:54.690Z"
   },
   {
    "duration": 838,
    "start_time": "2025-02-16T19:08:06.437Z"
   },
   {
    "duration": 11,
    "start_time": "2025-02-16T19:08:07.278Z"
   },
   {
    "duration": 18,
    "start_time": "2025-02-16T19:08:07.291Z"
   },
   {
    "duration": 6,
    "start_time": "2025-02-16T19:08:07.312Z"
   },
   {
    "duration": 17,
    "start_time": "2025-02-16T19:08:07.320Z"
   },
   {
    "duration": 29,
    "start_time": "2025-02-16T19:08:07.339Z"
   },
   {
    "duration": 60,
    "start_time": "2025-02-16T19:08:07.371Z"
   },
   {
    "duration": 177,
    "start_time": "2025-02-16T19:08:07.433Z"
   },
   {
    "duration": 17,
    "start_time": "2025-02-16T19:08:07.612Z"
   },
   {
    "duration": 176,
    "start_time": "2025-02-16T19:08:07.633Z"
   },
   {
    "duration": 330,
    "start_time": "2025-02-16T19:36:16.064Z"
   },
   {
    "duration": 86,
    "start_time": "2025-02-16T19:36:25.639Z"
   },
   {
    "duration": 975,
    "start_time": "2025-02-16T19:38:37.183Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-16T19:38:38.161Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-16T19:38:38.162Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-16T19:38:38.164Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-16T19:38:38.165Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-16T19:38:38.166Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-16T19:38:38.167Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-16T19:38:38.169Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-16T19:38:38.170Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-16T19:38:38.171Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-16T19:38:38.172Z"
   },
   {
    "duration": 972,
    "start_time": "2025-02-16T19:39:00.248Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-16T19:39:01.222Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-16T19:39:01.224Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-16T19:39:01.225Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-16T19:39:01.227Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-16T19:39:01.228Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-16T19:39:01.229Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-16T19:39:01.230Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-16T19:39:01.231Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-16T19:39:01.232Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-16T19:39:01.233Z"
   },
   {
    "duration": 812,
    "start_time": "2025-02-16T19:39:32.913Z"
   },
   {
    "duration": 9,
    "start_time": "2025-02-16T19:39:33.727Z"
   },
   {
    "duration": 12,
    "start_time": "2025-02-16T19:39:33.738Z"
   },
   {
    "duration": 5,
    "start_time": "2025-02-16T19:39:33.753Z"
   },
   {
    "duration": 14,
    "start_time": "2025-02-16T19:39:33.760Z"
   },
   {
    "duration": 51,
    "start_time": "2025-02-16T19:39:33.777Z"
   },
   {
    "duration": 18,
    "start_time": "2025-02-16T19:39:33.830Z"
   },
   {
    "duration": 185,
    "start_time": "2025-02-16T19:39:33.850Z"
   },
   {
    "duration": 81,
    "start_time": "2025-02-16T19:39:34.038Z"
   },
   {
    "duration": 103,
    "start_time": "2025-02-16T19:39:34.124Z"
   },
   {
    "duration": 196,
    "start_time": "2025-02-16T19:39:34.232Z"
   },
   {
    "duration": 824,
    "start_time": "2025-02-18T01:34:47.569Z"
   },
   {
    "duration": 9,
    "start_time": "2025-02-18T01:34:48.395Z"
   },
   {
    "duration": 12,
    "start_time": "2025-02-18T01:34:48.406Z"
   },
   {
    "duration": 6,
    "start_time": "2025-02-18T01:34:48.419Z"
   },
   {
    "duration": 21,
    "start_time": "2025-02-18T01:34:48.428Z"
   },
   {
    "duration": 27,
    "start_time": "2025-02-18T01:34:48.451Z"
   },
   {
    "duration": 65,
    "start_time": "2025-02-18T01:34:48.479Z"
   },
   {
    "duration": 130,
    "start_time": "2025-02-18T01:34:48.545Z"
   },
   {
    "duration": 68,
    "start_time": "2025-02-18T01:34:48.676Z"
   },
   {
    "duration": 107,
    "start_time": "2025-02-18T01:34:48.746Z"
   },
   {
    "duration": 191,
    "start_time": "2025-02-18T01:34:48.855Z"
   },
   {
    "duration": 66,
    "start_time": "2025-02-18T01:51:21.938Z"
   },
   {
    "duration": 53,
    "start_time": "2025-02-18T01:52:05.196Z"
   },
   {
    "duration": 469,
    "start_time": "2025-02-18T01:56:21.343Z"
   },
   {
    "duration": 930,
    "start_time": "2025-02-18T01:56:44.533Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-18T01:58:45.011Z"
   },
   {
    "duration": 933,
    "start_time": "2025-02-18T01:58:50.707Z"
   },
   {
    "duration": 902,
    "start_time": "2025-02-18T02:00:27.510Z"
   },
   {
    "duration": 937,
    "start_time": "2025-02-18T02:04:32.761Z"
   },
   {
    "duration": 910,
    "start_time": "2025-02-18T02:06:25.217Z"
   },
   {
    "duration": 812,
    "start_time": "2025-02-18T02:07:31.159Z"
   },
   {
    "duration": 9,
    "start_time": "2025-02-18T02:07:31.973Z"
   },
   {
    "duration": 12,
    "start_time": "2025-02-18T02:07:31.986Z"
   },
   {
    "duration": 6,
    "start_time": "2025-02-18T02:07:32.001Z"
   },
   {
    "duration": 35,
    "start_time": "2025-02-18T02:07:32.008Z"
   },
   {
    "duration": 27,
    "start_time": "2025-02-18T02:07:32.044Z"
   },
   {
    "duration": 21,
    "start_time": "2025-02-18T02:07:32.072Z"
   },
   {
    "duration": 130,
    "start_time": "2025-02-18T02:07:32.145Z"
   },
   {
    "duration": 68,
    "start_time": "2025-02-18T02:07:32.276Z"
   },
   {
    "duration": 125,
    "start_time": "2025-02-18T02:07:32.347Z"
   },
   {
    "duration": 181,
    "start_time": "2025-02-18T02:07:32.475Z"
   },
   {
    "duration": 955,
    "start_time": "2025-02-18T02:07:32.658Z"
   },
   {
    "duration": 936,
    "start_time": "2025-02-18T02:08:17.771Z"
   },
   {
    "duration": 936,
    "start_time": "2025-02-18T02:08:48.475Z"
   },
   {
    "duration": 946,
    "start_time": "2025-02-18T02:12:50.340Z"
   },
   {
    "duration": 258,
    "start_time": "2025-02-18T02:14:10.901Z"
   },
   {
    "duration": 42,
    "start_time": "2025-02-18T02:14:53.570Z"
   },
   {
    "duration": 753,
    "start_time": "2025-02-18T02:15:50.919Z"
   },
   {
    "duration": 10,
    "start_time": "2025-02-18T02:15:51.675Z"
   },
   {
    "duration": 12,
    "start_time": "2025-02-18T02:15:51.686Z"
   },
   {
    "duration": 5,
    "start_time": "2025-02-18T02:15:51.700Z"
   },
   {
    "duration": 35,
    "start_time": "2025-02-18T02:15:51.708Z"
   },
   {
    "duration": 27,
    "start_time": "2025-02-18T02:15:51.745Z"
   },
   {
    "duration": 19,
    "start_time": "2025-02-18T02:15:51.774Z"
   },
   {
    "duration": 132,
    "start_time": "2025-02-18T02:15:51.843Z"
   },
   {
    "duration": 76,
    "start_time": "2025-02-18T02:15:51.977Z"
   },
   {
    "duration": 106,
    "start_time": "2025-02-18T02:15:52.055Z"
   },
   {
    "duration": 191,
    "start_time": "2025-02-18T02:15:52.163Z"
   },
   {
    "duration": 963,
    "start_time": "2025-02-18T02:15:52.356Z"
   },
   {
    "duration": 999,
    "start_time": "2025-02-18T02:16:28.508Z"
   },
   {
    "duration": 8605,
    "start_time": "2025-02-18T02:35:46.865Z"
   },
   {
    "duration": 261,
    "start_time": "2025-02-18T02:37:08.556Z"
   },
   {
    "duration": 919,
    "start_time": "2025-02-18T02:37:15.824Z"
   },
   {
    "duration": 2604,
    "start_time": "2025-02-18T02:37:25.357Z"
   },
   {
    "duration": 928,
    "start_time": "2025-02-18T02:37:47.881Z"
   },
   {
    "duration": 1641,
    "start_time": "2025-02-18T02:41:35.319Z"
   },
   {
    "duration": 1624,
    "start_time": "2025-02-18T02:42:45.586Z"
   },
   {
    "duration": 73,
    "start_time": "2025-02-18T02:54:49.781Z"
   },
   {
    "duration": 789,
    "start_time": "2025-02-18T02:55:27.743Z"
   },
   {
    "duration": 805,
    "start_time": "2025-02-18T02:56:38.583Z"
   },
   {
    "duration": 10,
    "start_time": "2025-02-18T02:56:39.390Z"
   },
   {
    "duration": 13,
    "start_time": "2025-02-18T02:56:39.401Z"
   },
   {
    "duration": 6,
    "start_time": "2025-02-18T02:56:39.416Z"
   },
   {
    "duration": 33,
    "start_time": "2025-02-18T02:56:39.423Z"
   },
   {
    "duration": 26,
    "start_time": "2025-02-18T02:56:39.458Z"
   },
   {
    "duration": 73,
    "start_time": "2025-02-18T02:56:39.486Z"
   },
   {
    "duration": 123,
    "start_time": "2025-02-18T02:56:39.560Z"
   },
   {
    "duration": 68,
    "start_time": "2025-02-18T02:56:39.684Z"
   },
   {
    "duration": 120,
    "start_time": "2025-02-18T02:56:39.756Z"
   },
   {
    "duration": 177,
    "start_time": "2025-02-18T02:56:39.878Z"
   },
   {
    "duration": 780,
    "start_time": "2025-02-18T02:56:40.058Z"
   },
   {
    "duration": 957,
    "start_time": "2025-02-18T02:56:40.843Z"
   },
   {
    "duration": 1641,
    "start_time": "2025-02-18T02:56:41.802Z"
   },
   {
    "duration": 26,
    "start_time": "2025-02-18T03:28:30.940Z"
   },
   {
    "duration": 312,
    "start_time": "2025-02-18T03:29:26.481Z"
   },
   {
    "duration": 323,
    "start_time": "2025-02-18T03:35:34.894Z"
   },
   {
    "duration": 5,
    "start_time": "2025-02-18T03:36:58.544Z"
   },
   {
    "duration": 309,
    "start_time": "2025-02-18T03:37:08.408Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-18T03:44:47.798Z"
   },
   {
    "duration": 827,
    "start_time": "2025-02-18T03:51:44.899Z"
   },
   {
    "duration": 10,
    "start_time": "2025-02-18T03:51:45.728Z"
   },
   {
    "duration": 23,
    "start_time": "2025-02-18T03:51:45.739Z"
   },
   {
    "duration": 6,
    "start_time": "2025-02-18T03:51:45.776Z"
   },
   {
    "duration": 15,
    "start_time": "2025-02-18T03:51:45.784Z"
   },
   {
    "duration": 30,
    "start_time": "2025-02-18T03:51:45.801Z"
   },
   {
    "duration": 47,
    "start_time": "2025-02-18T03:51:45.833Z"
   },
   {
    "duration": 175,
    "start_time": "2025-02-18T03:51:45.883Z"
   },
   {
    "duration": 81,
    "start_time": "2025-02-18T03:51:46.069Z"
   },
   {
    "duration": 44,
    "start_time": "2025-02-18T03:51:46.163Z"
   },
   {
    "duration": 166,
    "start_time": "2025-02-18T03:51:46.211Z"
   },
   {
    "duration": 767,
    "start_time": "2025-02-18T03:51:46.379Z"
   },
   {
    "duration": 1008,
    "start_time": "2025-02-18T03:51:47.163Z"
   },
   {
    "duration": 1696,
    "start_time": "2025-02-18T03:51:48.174Z"
   },
   {
    "duration": 325,
    "start_time": "2025-02-18T03:51:49.872Z"
   },
   {
    "duration": 828,
    "start_time": "2025-02-18T03:52:35.941Z"
   },
   {
    "duration": 10,
    "start_time": "2025-02-18T03:52:36.772Z"
   },
   {
    "duration": 13,
    "start_time": "2025-02-18T03:52:36.784Z"
   },
   {
    "duration": 6,
    "start_time": "2025-02-18T03:52:36.799Z"
   },
   {
    "duration": 14,
    "start_time": "2025-02-18T03:52:36.808Z"
   },
   {
    "duration": 72,
    "start_time": "2025-02-18T03:52:36.823Z"
   },
   {
    "duration": 22,
    "start_time": "2025-02-18T03:52:36.897Z"
   },
   {
    "duration": 147,
    "start_time": "2025-02-18T03:52:36.947Z"
   },
   {
    "duration": 55,
    "start_time": "2025-02-18T03:52:37.096Z"
   },
   {
    "duration": 129,
    "start_time": "2025-02-18T03:52:37.153Z"
   },
   {
    "duration": 189,
    "start_time": "2025-02-18T03:52:37.284Z"
   },
   {
    "duration": 761,
    "start_time": "2025-02-18T03:52:37.476Z"
   },
   {
    "duration": 967,
    "start_time": "2025-02-18T03:52:38.239Z"
   },
   {
    "duration": 1706,
    "start_time": "2025-02-18T03:52:39.209Z"
   },
   {
    "duration": 337,
    "start_time": "2025-02-18T03:52:40.917Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
